{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "# Exercise Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWC5tKyP3x1e"
   },
   "source": [
    "Written by Morgan Schwartz and David Van Valen.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Some code cells will be marked with \n",
    "```\n",
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "```\n",
    "\n",
    "This indicates that you are being asked to write a piece of code to complete the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical machine learning methods often turn to manual feature engineering to extract elements of the data that the model will use for prediction. So far we have relied on the raw data alone, but in some cases well designed features can produce a better model than raw data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DwKG_Gi3x1f"
   },
   "outputs": [],
   "source": [
    "import imageio as iio\n",
    "import skimage\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm.auto\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pkm3aspYX_WZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup\n",
    "You need to run all of the code cells from here down to the \"Introduction to image filters\" section. These blocks of code were directly taken from the prvious notebook so feel free to skip down to the image filter section once you have started them running to load the dataset.\n",
    "\n",
    "### Load dataset\n",
    "We will load the same dataset as the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/CellCycle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe with sample info\n",
    "df = pd.read_csv(os.path.join(data_dir, 'img.lst'), sep='\\t', header=None)\n",
    "df = df.rename(columns={1: 'class', 2: 'filepath'})\n",
    "df['channel'] = df['filepath'].str.split('/',expand=True)[2].str.split('_', expand=True)[1].str.slice(2,3)\n",
    "df['id'] = df['filepath'].str.split('/',expand=True)[2].str.split('_', expand=True)[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data stack\n",
    "ims = []\n",
    "ys = []\n",
    "for i, g in df.groupby('id'):\n",
    "    im = []\n",
    "    for _, r in g.iterrows():\n",
    "        im.append(iio.imread(os.path.join(data_dir, r['filepath'])))\n",
    "    ims.append(np.stack(im, axis=-1))\n",
    "    ys.append(r['class'])\n",
    "    \n",
    "X_data = np.stack(ims)\n",
    "y_data = np.stack(ys)\n",
    "print('X shape:', X_data.shape)\n",
    "print('y shape:', y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes(X_data, y_data, classes):\n",
    "    \"\"\"For a given dataset of categorical labels, this data\n",
    "    selects the datapoints associated with the classes of interest\n",
    "    and returns them stacked in a new array\n",
    "    \n",
    "    Args:\n",
    "        X_data (np.array): Array of x data\n",
    "        y_data (np.array): Array of categorical y data\n",
    "        classes (list): List of categorical classes to select\n",
    "        \n",
    "    Returns:\n",
    "        np.array: X data after extracting the classes of interest \n",
    "        np.array: y data after selecting the classes of interest\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for c in classes:\n",
    "        # Identify the indicies of the relevant class\n",
    "        idx = y_data == c\n",
    "        # Select the X and y data accordingly\n",
    "        X.append(X_data[idx, ..., 0:1])\n",
    "        y.append(y_data[idx])\n",
    "\n",
    "    # Restack the arrays\n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_classes(X_data, y_data, [4, 5])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == 4] = 0\n",
    "y[y == 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "842TuxybX_WX"
   },
   "outputs": [],
   "source": [
    "def balance_classes(X, y, minority_id):\n",
    "    \"\"\"For a given minority class id, upsample the minority class\n",
    "    to match the number of samples in the majority class\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Array of raw data\n",
    "        y (np.array): Array of class labels\n",
    "        minority_id (int): Integer of the minority class to be upsampled\n",
    "        \n",
    "    Returns:\n",
    "        np.array: X\n",
    "        np.array: y\n",
    "    \"\"\"\n",
    "    # Split the X and y arrays into sub arrays containing \n",
    "    # 1) only the minority samples and 2) the remaining samples\n",
    "    x_max = X[y != minority_id]\n",
    "    y_max = y[y != minority_id]\n",
    "    x_min = X[y == minority_id]\n",
    "    y_min = y[y == minority_id]\n",
    "    \n",
    "    # Use sklearn.utils.resample to samples from the minority sample array \n",
    "    # to match the number of samples in the second array\n",
    "    x_min, y_min = sklearn.utils.resample(x_min, y_min, n_samples=x_max.shape[0])\n",
    "    \n",
    "    # Concatenate the upsampled array 1 with the remaining array 2\n",
    "    X = np.concatenate([x_max, x_min])\n",
    "    y = np.concatenate([y_max, y_min])\n",
    "    \n",
    "    # Shuffle arrays to randomize sample order\n",
    "    X, y = sklearn.utils.shuffle(X, y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create dataset builder\n",
    "def build_dataset(X, y, batch_size=1, seed=1, train_size=0.8):\n",
    "    # Create train/test splits\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        X, y, \n",
    "        train_size=train_size, \n",
    "        random_state=seed)\n",
    "    \n",
    "    # Balance classes in each split\n",
    "    X_train, y_train = balance_classes(X_train, y_train, 1)\n",
    "    X_test, y_test = balance_classes(X_test, y_test, 1)\n",
    "    \n",
    "    # Convert y data to categorical\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(256).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-jix6tiX_WZ",
    "outputId": "a02646e0-7172-47c4-d57c-bdfb75e98ff9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the linear classifier\n",
    "def create_linear_classifier():\n",
    "    \"\"\"Defines a basic linear classifier to predict 2 classes and \n",
    "    compiles the model with the following:\n",
    "    - Optimizer = Adam\n",
    "    - Loss = Categorical Crossentropy\n",
    "    - Metrics = Recall and precision for each class\n",
    "    \"\"\"\n",
    "    inputs = Input((X.shape[1], X.shape[2], 1),\n",
    "                   name='linear_classifier_input')\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(2)(x)\n",
    "    x = Softmax(axis=-1)(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3, clipnorm=0.001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics = [\n",
    "                      tf.keras.metrics.Recall(class_id=0),\n",
    "                      tf.keras.metrics.Recall(class_id=1),\n",
    "                      tf.keras.metrics.Precision(class_id=0),\n",
    "                      tf.keras.metrics.Precision(class_id=1)\n",
    "                  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veiaElEQX_Wb"
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "n_epochs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, n_epochs, model_path):\n",
    "    \"\"\"Fits a model using data from a `DatasetBuilder`. \n",
    "    The weights of the model are saved in a folder according\n",
    "    to `model_path`\n",
    "    \n",
    "    Args:\n",
    "        model (tensorflow model): Model from `create_linear_classifier`\n",
    "        data (DatasetBuilder): Dataset builder object \n",
    "        n_epochs (int): Number of epochs to train for\n",
    "        model_path (str): Path to save model weights\n",
    "    \"\"\"\n",
    "    model.fit(\n",
    "        data['train'],\n",
    "        epochs=n_epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(y_true, y_pred):\n",
    "    \"\"\"Calculates recall, precision, f1 and a confusion matrix for sample predictions\n",
    "    \n",
    "    Args:\n",
    "        y_true (list): List of integers of true class values\n",
    "        y_pred (list): List of integers of predicted class value\n",
    "        cm_norm (optional): 'true' to return a normalized confusion matrix.\n",
    "            None to return an raw confusion matrix\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary with keys `recall`, `precision`, `f1`, and `cm`\n",
    "    \n",
    "    \"\"\"\n",
    "    _round = lambda x: round(x, 3)\n",
    "    \n",
    "    metrics = {\n",
    "        'recall': _round(sklearn.metrics.recall_score(y_true, y_pred)),\n",
    "        'precision': _round(sklearn.metrics.precision_score(y_true, y_pred)),\n",
    "        'f1': _round(sklearn.metrics.f1_score(y_true, y_pred)),\n",
    "        'cm': sklearn.metrics.confusion_matrix(y_true, y_pred, normalize=None),\n",
    "        'cm_norm': sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, name, ax=None):\n",
    "    \"\"\"Plots a confusion matrix with summary statistics listed above the plot\n",
    "    \n",
    "    The annotations on the confusion matrix are the total counts while\n",
    "    the colormap represents those counts normalized to the total true items\n",
    "    in that class.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Dictionary output of `benchmark_performance`\n",
    "        name (str): Title for the plot\n",
    "        ax (optional, matplotlib subplot): Subplot axis to plot onto. \n",
    "            If not provided, a new plot is created\n",
    "        classes (optional, list): A list of the classes to label the X and y \n",
    "            axes. Defaults to [0, 1] for a two class problem.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "    cb = ax.imshow(metrics['cm_norm'], cmap='Greens', vmin=0, vmax=1)\n",
    "    \n",
    "    classes = np.arange(metrics['cm'].shape[0])\n",
    "    plt.xticks(range(len(classes)), classes)\n",
    "    plt.yticks(range(len(classes)), classes)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            color='green' if metrics['cm_norm'][i,j] < 0.5 else 'white'\n",
    "            ax.annotate('{}'.format(metrics['cm'][i,j]), (j, i),\n",
    "                        color=color, va='center', ha='center')\n",
    "\n",
    "    _ = plt.colorbar(cb, ax=ax)\n",
    "    _ = ax.set_title(\n",
    "            '{}\\n'\\\n",
    "            'Recall: {}\\n'\\\n",
    "            'Precision: {}\\n'\\\n",
    "            'F1 Score: {}\\n'\\\n",
    "            ''.format(name, metrics['recall'], metrics['precision'], metrics['f1'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to image filters\n",
    "\n",
    "Image filters operate by taking a small kernel (or matrix) and applying it to each pixel in the image to compute a new value. As an example, this is the identity kernel \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "which when applied to an image will not result in any changes to the data.\n",
    "\n",
    "Filters can produce a variety of effects on images depending on how the kernel is configured. This can range from blurring an image to extracting edges. Filtered images can contain data that is more informative to the model when distinguishing between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = X[np.random.randint(X.shape[0]),...,0]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "ax[0].imshow(im)\n",
    "ax[0].set_title('Original')\n",
    "\n",
    "ax[1].imshow(skimage.filters.gaussian(im))\n",
    "ax[1].set_title('Gaussian')\n",
    "\n",
    "ax[2].imshow(skimage.filters.laplace(im))\n",
    "ax[2].set_title('Laplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of a filter can also be influenced by changing the sigma parameter which modifies the size of the kernel, as shown below for the Gaussian filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 4))\n",
    "sigmas = [1, 2, 5, 10]\n",
    "\n",
    "for i, s in enumerate(sigmas):\n",
    "    ax[i].imshow(skimage.filters.gaussian(im, sigma=s))\n",
    "    ax[i].set_title('Sigma = {}'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of filters are made available through the `skimage.filters` [module](https://scikit-image.org/docs/stable/api/skimage.filters.html). In this part of the exercise, we are going to explore how filters can be applied to images in order to extract features for model prediction. While we are going to work with the filters that are easily available through skimage, there are many other transformations that can be applied to images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "\n",
    "# List all functions in the skimage.filters module to get a list of available filters\n",
    "[m[0] for m in getmembers(skimage.filters, isfunction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 3.1\n",
    "\n",
    "Test a variety of the available filters from the `skimage` module. Whenever you are making a modification to an image, you should check the results to make sure that errors are not introduced while generating the transformation. \n",
    "\n",
    "Ultimately we are going to use model performance to select the best features for our classification task, but you should be familiar with the output of any filters that you are using. The goal of this next section is to identify a set of candidate filters from which one will be chosen that you think will lead to better classification results on the two classes we are trying to distinguish.\n",
    "\n",
    "Keep the following things you may want to keep in mind as you approach this problem\n",
    "- Look at several randomly selected images from the two classes when you are testing a filter\n",
    "- Explore the effect of parameters available for each filter\n",
    "\n",
    "**Challenge**: While this task could be approached by testing filters one at a time, consider writing a for loop to rapidly test filters in an automated fashion.\n",
    "\n",
    "\n",
    "**Tip**: Within a jupyter notebook, you can run `function?` to look at the documentation for that function. Check out the example below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.filters.gaussian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Put your code for testing filters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 2</h2>\n",
    "    \n",
    "Please 👍 the \"Checkpoint 2\" slack thread when you reach this checkpoint.\n",
    "\n",
    "We'll discuss as a group which filters seem like they may be most effective at distinguishing our two classes. Please be prepared with your top two or three candidates. \n",
    "\n",
    "*Bonus:* Collect screenshots of your filtering results to share with the group. Look out for a slack message for instructions on how to submit images.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 3.2\n",
    "Starting with a few of your top choices for filters, train a model on each filter variation and compare the results. Again as an extra challenge, you can write a for loop to automate this process.\n",
    "\n",
    "You will roughly need to follow these steps:\n",
    "- Create a new version of `X` and `y` with the filter applied. Make sure that you are working on a new copy of the original data each time you apply a filter. You can use `copy.deepcopy` to create a new copy of an array.\n",
    "- Create a new `DatasetBuilder` object with the filtered images\n",
    "    ```\n",
    "    with tf.device('CPU:0'):\n",
    "        dataset = build_dataset(X, y, batch_size=64, seed=seed, train_size=train_size)\n",
    "    ```\n",
    "- Train a model using the functions `create_linear_classifier` and `train_model` that are defined in an earlier section of the notebook\n",
    "- Benchmark and collect the metrics for each model variant \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to use these standard parameters from the previous notebook\n",
    "seed = 10\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Your code for training and comparing models here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint</h2>\n",
    "    \n",
    "Please 👍 the \"Checkpoint 2\" slack thread when you reach this checkpoint.\n",
    "\n",
    "We'll discuss as a group what filters worked best to distinguish between the two classes. When you're ready, you can submit your best model to this [google form](https://docs.google.com/forms/d/e/1FAIpQLSfVblJCnprXct0IrS87cGw4ijpH2h6bKvcXqj7RNraMrSP3PA/viewform?usp=sf_link). \n",
    "    \n",
    "</div>\n",
    "\n",
    "## Bonus\n",
    "\n",
    "If you have extra time, try one of the following challenges:\n",
    "- How does your selected filter perform on another pair of classes in this dataset?\n",
    "- Can you automate the search for the top performing filter by writing a function that ultimately reports the top performer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:01_intro_ml]",
   "language": "python",
   "name": "conda-env-01_intro_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
